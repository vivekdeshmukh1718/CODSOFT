{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yKe88-dgRlB",
        "outputId": "fa242469-5084-4fe3-d066-11f563649129"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_tags.py:354: FutureWarning: The SMOTE or classes from which it inherits use `_get_tags` and `_more_tags`. Please define the `__sklearn_tags__` method, or inherit from `sklearn.base.BaseEstimator` and/or other appropriate mixins such as `sklearn.base.TransformerMixin`, `sklearn.base.ClassifierMixin`, `sklearn.base.RegressorMixin`, and `sklearn.base.OutlierMixin`. From scikit-learn 1.7, not defining `__sklearn_tags__` will raise an error.\n",
            "  warnings.warn(\n",
            "Applying Incremental PCA: 100%|██████████| 10/10 [00:09<00:00,  1.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9964007198560288\n",
            "Confusion Matrix:\n",
            "[[4983    0]\n",
            " [  18    0]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      4983\n",
            "           1       0.00      0.00      0.00        18\n",
            "\n",
            "    accuracy                           1.00      5001\n",
            "   macro avg       0.50      0.50      0.50      5001\n",
            "weighted avg       0.99      1.00      0.99      5001\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Hello again everyone! This is Darshan S\n",
        "Intern at CODSOFT, India.\n",
        "\n",
        "I'm very thrilled to share my Second Task at CodSoft Internship September 2023 viz. CREDIT CARD FRAUD DETECTION MODEL.\n",
        "More details on the structure, working and requirements are available in the README files of respective folder name corresponding to the Task Name.\n",
        "'''\n",
        "\n",
        "# AUTHOR: DARSHAN S\n",
        "# TASK NAME: Credit Card Fraud Detection\n",
        "# 2nd Task in the List of Tasks\n",
        "# TASK CATEGORY: Machine Learning\n",
        "# DATE OF SUBMISSION: 26 September 2023\n",
        "# LinkedIn Profile: https://linkedin.com/in/arcticblue/\n",
        "# GitHub Repository: https://github.com/azuregray/CodSoft-InternshipTasks/\n",
        "\n",
        "\n",
        "# Lets start the code by importing necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.decomposition import IncrementalPCA\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load training dataset\n",
        "train_data = pd.read_csv(\"fraudTrain.csv\")\n",
        "\n",
        "# Load testing dataset\n",
        "test_data = pd.read_csv(\"fraudTest.csv\")\n",
        "\n",
        "# Combine training and testing data for encoding consistency\n",
        "combined_data = pd.concat([train_data, test_data], axis=0)\n",
        "\n",
        "# Extract relevant features from the \"trans_date_trans_time\" column\n",
        "def extract_datetime_features(df):\n",
        "    df['trans_date_trans_time'] = pd.to_datetime(df['trans_date_trans_time'])\n",
        "    df['day_of_week'] = df['trans_date_trans_time'].dt.dayofweek\n",
        "    df['hour_of_day'] = df['trans_date_trans_time'].dt.hour\n",
        "    df.drop('trans_date_trans_time', axis=1, inplace=True)\n",
        "    return df\n",
        "\n",
        "combined_data = extract_datetime_features(combined_data)\n",
        "\n",
        "# Drop irrelevant columns (you can customize this based on your data)\n",
        "columns_to_drop = [\"first\", \"last\", \"job\", \"dob\", \"trans_num\", \"street\"]\n",
        "combined_data.drop(columns_to_drop, axis=1, inplace=True)\n",
        "\n",
        "# Separate features and target variable\n",
        "X_combined = combined_data.drop(\"is_fraud\", axis=1)\n",
        "y_combined = combined_data[\"is_fraud\"]\n",
        "\n",
        "# Encode the \"merchant\" and \"category\" columns using LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "X_combined[\"merchant\"] = label_encoder.fit_transform(X_combined[\"merchant\"])\n",
        "X_combined[\"category\"] = label_encoder.fit_transform(X_combined[\"category\"])\n",
        "\n",
        "# One-hot encode categorical variables with handle_unknown='ignore'\n",
        "categorical_columns = [\"gender\", \"city\", \"state\"]\n",
        "onehot_encoder = OneHotEncoder(sparse_output=False, drop=\"first\", handle_unknown='ignore')\n",
        "\n",
        "X_combined_categorical = onehot_encoder.fit_transform(X_combined[categorical_columns])\n",
        "\n",
        "# Standardize the numeric features\n",
        "scaler = StandardScaler()\n",
        "X_combined_numeric = scaler.fit_transform(X_combined.drop(categorical_columns, axis=1))\n",
        "\n",
        "# Combine one-hot encoded categorical features with numeric features\n",
        "X_combined_encoded = np.hstack((X_combined_numeric, X_combined_categorical))\n",
        "\n",
        "# Split the combined data back into training and test datasets\n",
        "X_train = X_combined_encoded[:len(train_data)]\n",
        "X_test = X_combined_encoded[len(train_data):]\n",
        "y_train = y_combined[:len(train_data)]\n",
        "y_test = y_combined[len(train_data):]\n",
        "\n",
        "# Address class imbalance using SMOTE on the training data\n",
        "sm = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = sm.fit_resample(X_train, y_train)\n",
        "\n",
        "# Apply Incremental PCA for dimensionality reduction\n",
        "n_components = 100  # Adjust the number of components as needed\n",
        "ipca = IncrementalPCA(n_components=n_components)\n",
        "\n",
        "# Apply Incremental PCA to training data with progress bar\n",
        "for batch in tqdm(np.array_split(X_resampled, 10), desc=\"Applying Incremental PCA\"):\n",
        "    ipca.partial_fit(batch)\n",
        "\n",
        "# Transform the training and testing data\n",
        "X_resampled_pca = ipca.transform(X_resampled)\n",
        "X_test_pca = ipca.transform(X_test)\n",
        "\n",
        "# Define and train the Random Forest model\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Fit the model with the resampled data\n",
        "rf_classifier.fit(X_resampled_pca, y_resampled)\n",
        "\n",
        "# Predict using the trained model\n",
        "y_pred = rf_classifier.predict(X_test_pca)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "# Display the necessary report on ML Model Metrics for evaluation\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Confusion Matrix:\\n{confusion}\")\n",
        "print(f\"Classification Report:\\n{report}\")"
      ]
    }
  ]
}